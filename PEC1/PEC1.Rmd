---
title: "PEC1"
author: "María Sánchez Paniagua"
date: "2024-04-23"
output:
  prettydoc::html_pretty:
    theme: tactile
    highlight: vignette
    toc: true
    toc_depth: 3
    number_sections: true
    #code_folding: hide
    bibliography: ADOreferences.bib
    editor_options: 
      #chunk_output_type: console
      chunk_output_type: inline

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(max.print="60")

library(knitr)
library(lmtest)
library(nortest)
library(car)
library(MASS)

opts_chunk$set(echo=TRUE,
	             cache=FALSE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)




#palette <- brewer.pal(n = 20, name = "Set2")
```


# Soluciones de otras actividades:

https://aula.uoc.edu/courses/36506/assignments/407649?module_item_id=1390767

https://fhernanb.github.io/libro_regresion/diag2.html


# Ejercicio 1

The Framingham Heart Study (Levy, 1999) recogió datos sobre los factores de riesgo cardiovascular y el
seguimiento a largo plazo de casi 5.000 residentes de la ciudad de Framingham (Massachusetts).
La muestra se compone de 4240 individuos, 1944 hombres y 2490 mujeres y 16 variables. Aunque el
objetivo del estudio era predecir si el paciente tenía un riesgo a 10 años de sufrir en el futuro una
cardiopatía coronaria, en este ejercicio se va a intentar predecir la presión sanguínea sistólica, factor de
riesgo cardiovasvular, a partir de algunas variables registradas.
Las variables que se utilizarán en los ejercicios son las siguientes:


sysBP (Systolic blood pressure): Presión arterial sistólica variable Y que se pretende predecir.
BMI (Body mass index): Índice de masa corporal
age: edad
male: sexo (1: varón; 0: mujer)
totChol: colesterol total
heartRate: pulsaciones por minuto
currentSmoker: Fumadxr (1: fumadxr; 0: no fumadxr)
cigsPerDay: cigarrillos por día
diabetes: (0:no; 1:sí)
glucose: glucosa en sangre

(a) Estimar un modelo de regresión lmod_inicial que permita obtener la influencia, si existe, únicamente
del índice de masa corporal (BMI) sobre la presión arterial sistólica (sysBP) y si esta relación
varía con el sexo (male). Comentar el resultado de la regresión, en cuanto a la relación y si ésta
varía con el sexo. Comentar también si la bondad de ajuste lineal es suficiente o si es necesario
incluir más variables para explicar la varianza de sysBP.
Nota: Se recomienda eliminar las observaciones con valores faltantes (missings).

```{r}
# Cargar datos y eliminar valores faltantes
data_framingham <- read.csv("C:/Users/Usuario/GitHub/MASTER_Regresion_mod_y_met/PEC1/framingham.csv")
data_framingham <- na.omit(data_framingham)
head(data_framingham)

# Primer modelo de regresión lineal
lmod_inicial <- lm(sysBP ~ BMI + male + BMI:male, data = data_framingham)

# Ver el resumen del modelo
summary(lmod_inicial)
```

Todos los coeficientes (incluso el de la relación de las dos variebles) son menores de 0,05 por lo que son significativos. El intercepto negativo indica que el efecto de del BMI en la presión sistólica es menor que en mujeres.

El valor de R-cuadrado ajustado es de 0,11 por lo que es bastante bajo, explica poca variabilidad por lo que sería interesante añadir más variables.

Además, el modelo como tal también es significativo, ya que el valor de p del estadístico F es muy bajo.

A continuación estos serán los gráficos del modelo:

```{r}
plot(lmod_inicial)
```


(b) Dibujar un gráfico de dispersión con las rectas de regresión de hombres y mujeres según el modelo
del apartado anterior.

```{r}
with(data_framingham, {
  plot(BMI, sysBP, col = ifelse(male == 1, "orange", "hotpink"), xlab = "Índice de Masa Corporal (BMI)", ylab = "Presión Sanguínea Sistólica (sysBP)", main = "Regresión de sysBP sobre BMI por Sexo")
  legend("topright", legend = c("Hombres", "Mujeres"), col = c("orange", "hotpink"), pch = 16)
  abline(lm(sysBP ~ BMI, data = data_framingham, subset = male == 0), col = "hotpink")
  abline(lm(sysBP ~ BMI, data = data_framingham, subset = male == 1), col = "orange")
})

```


(c) Hallar los intervalos de confianza al 99% para los coeficientes del modelo del apartado (a).
Calcular una estimación de la varianza del error en el mismo modelo.

```{r}
# Intervalos de confianza para los coeficientes del modelo inicial
confint(lmod_inicial, level = 0.99)

# Estimación de la varianza del error
sigma_squared <- sum(residuals(lmod_inicial)^2) / lmod_inicial$df.residual
sigma_squared

```
Los intervalos de confianza para la variable BMI es (74.137826, 87.7444529), los de sexo (1.787986,  2.3130589) y los de la interacción (-1.197452, -0.2543371).

(d) Además de las variables contempladas anteriormente, se cree que hay otras variables clínicas y
demográficas que predicen linealmente la presión arterial sistólica como son la edad, el colesterol
total, las pulsaciones por minuto, si se es o no fumador y el número de cigarrillos por día, si se
es diábetico y la glucosa en sangre. Como se ha visto en los apartados anteriores la relación entre
sysBP y BMI puede depender del sexo.
Estimar un modelo de regresión lineal múltiple lmod_ampliado para predecir sysBP que tenga como
variables predictoras: BMI, age, sexo, totChol, heartRate, currentSmoker, cigsPerDay, diabetes,
glucose y la interacción BMI con sexo.



¿Es significativo el modelo obtenido? Plantear la hipótesis nula y la alternativa del test. ¿Qué test
estadístico se emplea para contestar a esta pregunta?
Explicar el resultado del coeficiente de la regresora cigsPerDay y su significación en el contexto de
este modelo.


```{r}
lmod_ampliado <- lm(sysBP ~ BMI + age + male + totChol + heartRate + currentSmoker + cigsPerDay + diabetes + glucose + BMI:male, data = data_framingham)

# Resumen del modelo ampliado
summary(lmod_ampliado)
```
Para conocer la significación del modelo realizamos un ANOVA. 
La hipótesis nula sería que no hay relación lineal significativa entre las variables predictoras y la presión arterial sistólica (sysBP). La hipótesis alternativa sería que al menos una de las variables predictoras tiene una relación lineal significativa con sysBP.

```{r}
anova(lmod_inicial, lmod_ampliado)
```


Respecto a la significación, el valor de p del estadístico F es prácticamente cero, lo que indica que el modelo en su conjunto es significativo. El estadóstico F del ANOVA nos permite comprobar la significancia del modelo completo. El F valor tan pequeño indica que hay una diferencia significativa entre los dos modelos.

Respecto a la variable cigsPerDar, el valor de 0.045974 sugiere que, en promedio, un aumento de un cigarrillo por día se asocia con un aumento de aproximadamente 0.046 unidades en la presión arterial sistólica, aunque este efecto no es estadísticamente significativo.

(e) Contrastar si nos podemos quedar con el modelo más reducido que no tiene en cuenta las variables
regresoras: currentSmoker, cigsPerDay y diabetes. Escribir en forma paramétrica las hipótesis del
test H0 y H1 de este contraste. Estimar un nuevo modelo lmod_reducido en el que no intervengan
estas variables.

```{r}
# Modelo reducido excluyendo 'currentSmoker', 'cigsPerDay', y 'diabetes'
lmod_reducido <- lm(sysBP ~ BMI + age + male + totChol + heartRate + glucose + BMI:male, data = data_framingham)
summary(lmod_reducido)

# Test ANOVA para comparar el modelo reducido y el ampliado
anova(lmod_reducido, lmod_ampliado)

```

Las hipótesis son:
\[H_0: \beta_{\text{currentSmoker}} = \beta_{\text{cigsPerDay}} = \beta_{\text{diabetes}} = 0\]

\[H_1: \text{Al menos uno de } \beta_{\text{currentSmoker}}, \beta_{\text{cigsPerDay}}, \text{ o } \beta_{\text{diabetes}} \neq 0\]

Según el ANOVA a ambos modelos no hay diferencias significativas y por tanto podemos obviar esas variables del modelo.


(f) Con el modelo elegido, calcular un intervalo de predicción al 95% de un individuo con los siguientes valores de la variables predictoras: BMI=29, age=64, male=0, totChol=200, heartRate=70,
glucose=96. Comprobar previamente que los valores observados no suponen una extrapolación.
Para ello utilizaremos el elipsoide que se forma con el leverage máximo.

Lo intento hacer con el chell

```{r}
data_framingham
# Obtener los valores ajustados por el modelo
fitted_values <- predict(lmod_reducido)

# Calcular el chull de los valores ajustados
chull_points <- chull(fitted_values)

# Verificar si los valores ajustados caen fuera del chull
if (length(chull_points) < length(fitted_values)) {
  warning("Extrapolación potencial: Algunos valores ajustados están fuera del chull.")
} else {
  print("No hay extraploación potencial: Todos los valores ajustados están dentro del chull.")
}

# Visualizar elipsoide de leverage máximo
plot(fitted_values, pch = 16, col = "blue", main = "Envolvente convexo de los valores ajustados")
polygon(c(fitted_values[chull_points], rev(fitted_values[chull_points])), c(rep(min(fitted_values), length(chull_points)), rep(max(fitted_values), length(chull_points))), col = "lightblue", border = NA)

```

La fórmula para calcular el leverage (\( h_i \)) de un punto en un modelo de regresión lineal es:

\[ h_i = x_i^T (X^T X)^{-1} x_i \]

Donde:

- \( x_i \) es el vector de predictores para el i-ésimo punto.
- \( X \) es la matriz de diseño (matriz de predictores) del conjunto de datos.

Para calcular el leverage en R, puedes utilizar la siguiente función:

```{r}
datos <- data_framingham[, c("BMI", "age", "male", "totChol", "heartRate", "glucose")]
X <- model.matrix(lmod_reducido)
nombres_columnas <- c("BMI", "age", "male", "totChol", "heartRate", "glucose")
nuevo_punto_df <- data.frame(Intercept = 1,BMI = 29, age = 64, male = 0, totChol = 200, heartRate = 70, glucose = 96, BMI_male = 29 * 0, stringsAsFactors = FALSE)
nuevo_punto <- as.numeric(nuevo_punto_df)

# Calculo leverage
t <- nuevo_punto
s <- solve(crossprod(X, X))
leverage <- crossprod(nuevo_punto, s %*% t)
as.numeric(leverage)



```
Lo hago con el leverage máximo:

```{r}
H<- hatvalues(lmod_reducido)
max_leverage <- max(H)
max_leverage


# Verificar si los valores dados están dentro del elipsoide formado por el leverage máximo
if (leverage > max_leverage) {
  warning("Extrapolación potencial: leverage máximo es mayor que el del punto")
} else {
  print("No hay extraploación potencial: leverage máximo es menor o igual que el del punto")
}


```

```{r}
# Usar el modelo reducido o el que se prefiera basado en análisis previos
predict(lmod_reducido, nuevo_punto, interval = "prediction", level = 0.95)

```

#**EL PROBEMA AQUI ES SYSBP**

```{r}
# Supongamos que estos son los nombres de las columnas para tu modelo y están en este orden en el DataFrame 'datos'

nombres_columnas <- c("sysBP", "BMI", "age", "male", "totChol", "heartRate", "glucose")


# Crear un DataFrame para el nuevo punto con los nombres de columnas adecuados y sin convertir en factor

# Asegúrate de que el nuevo punto tiene el mismo tipo de datos que el DataFrame original
nuevo_punto[] <- lapply(nuevo_punto, function(x) as.numeric(as.character(x)))

# Añadir el nuevo punto al DataFrame original
datos <- data_framingham[,nombres_columnas]
datos_nuevos <- rbind(datos, nuevo_punto)
names(datos)
# Verificar la estructura de datos nuevos para confirmar la consistencia
str(datos_nuevos)

# Ajustar el modelo con el nuevo conjunto de datos
modelo_nuevo <- lm(sysBP ~ BMI + age + male + totChol + heartRate + glucose, data = datos_nuevos)

# Revisar el modelo para asegurar que no hay errores
summary(modelo_nuevo)

```



# Ejercicio 2

En este ejercicio se pretende mejorar el modelo del ejercicio anterior haciendo algunas pruebas diagnósticas.


(a) Obtener con la función plot() los gráficos básicos de diagnóstico del modelo lmod_reducido del
apartado (e) del ejercicio anterior. Explicar los cuatro gráficos.
Comprobar con algún test las hipótesis de homocedasticidad y normalidad de los errores. Utilizar
dos tests distintos para cada una de las dos hipótesis.



```{r}
# Cargar el modelo reducido
# Suponiendo que el modelo se ha guardado en una variable llamada lmod_reducido

# Gráficos de diagnóstico
par(mfrow = c(2, 2))
plot(lmod_reducido)

# Test de homocedasticidad usando el test de Breusch-Pagan y el test de White

bptest(lmod_reducido)

# Test de normalidad usando el test de Shapiro-Wilk y el test de Anderson-Darling
shapiro.test(residuals(lmod_reducido))

ad.test(residuals(lmod_reducido))

```

Explicación de los gráficos:

Residuales vs Valores Ajustados: Este gráfico ayuda a verificar la homocedasticidad de los errores. Los residuos deben estar distribuidos de forma aleatoria y sin formar patrones.
Normal Q-Q: Este gráfico es crucial para verificar si los residuales siguen una distribución normal. Los puntos deben seguir la línea diagonal.
Scale-Location (o Spread-Location): Similar al primer gráfico, pero muestra la raíz cuadrada de los residuos estandarizados, proporcionando otra visualización de la homocedasticidad.
Residuals vs Leverage: Ayuda a identificar observaciones influyentes. Los puntos a tener en cuenta son aquellos que están lejos del centro de la horizontal (alta influencia).


(b) Realizar un estudio descriptivo de la variable sysBP, especialmente sobre su distribución. Buscar
una transformación de esta variable que mejore sus propiedades.
Sugerencia: Utilizar la transformación log(x + a) basándose en el modelo reducido obtenido en el
ejercicio anterior. Para ello, la función logtrans() del paquete MASS nos puede ayudar.

```{r}
# Estudio descriptivo de sysBP
summary(data_framingham$sysBP)
hist(data_framingham$sysBP, breaks = 30, main = "Histograma de sysBP", xlab = "sysBP")

# Transformación logarítmica sugerida

sysBP <- data_framingham$sysBP
transformed_sysBP <- logtrans(sysBP, lambda = 1)  # ajustar el valor de lambda según sea necesario

# Verificar la distribución transformada
hist(transformed_sysBP, breaks = 30, main = "Histograma de sysBP Transformada", xlab = "sysBP Transformada")


```

## c

(c) Estimar el modelo reducido con la transformación obtenida en el apartado anterior y comprobar si
se ha mejorado en la homocedasticidad y en la normalidad.


```{r}
# Estimación del modelo con la variable transformada
lmod_transformado <- lm(transformed_sysBP ~ predictors, data = dataset)  # Ajustar 'predictors' según las variables utilizadas

# Verificación de mejoras en homocedasticidad y normalidad
par(mfrow = c(2, 2))
plot(lmod_transformado)

```

## d

(d) Estudiar la presencia de valores atípicos, de alto leverage y/o puntos influyentes en el último modelo
con la variable respuesta transformada.
Dibujar un gráfico resumen.


```{r}
# Análisis de outliers y puntos con alto leverage
plot(lmod_transformado, which = 5)  # Gráfico de Cook's distance para identificar puntos influyentes
```


## e

(e) Hallar los tres puntos más influyentes del apartado anterior. ¿Son también atípicos (outliers)?
Estimar un nuevo modelo sin esos 3 puntos y comprobar otra vez la homocedasticidad y la normalidad

```{r}
# Identificar y remover los tres puntos más influyentes
influential <- cooks.distance(lmod_transformado) > 4 / nrow(dataset)  # ajustar según el tamaño de muestra
dataset_clean <- dataset[!influential, ]

# Re-estimar el modelo sin esos puntos
lmod_final <- lm(transformed_sysBP ~ predictors, data = dataset_clean)

# Verificación final de homocedasticidad y normalidad
par(mfrow = c(2, 2))
plot(lmod_final)

```



# Ejercicio 3 (20 pt.)

Con el modelo reducido del apartado (e) del ejercicio 2 contestar las siguientes cuestiones:
(a) Hallar la matriz del diseño del modelo. ¿Cual es su rango? ¿Coincide con el número de parámetros
βi? ¿Tendremos problemas para estimar combinaciones lineales de los βi?

```{r}
# Asumimos que el modelo reducido es lmod_reducido

# Obtener la matriz de diseño del modelo
design_matrix <- model.matrix(lmod_reducido)

# Calcular el rango de la matriz de diseño
matrix_rank <- qr(design_matrix)$rank

# Número de parámetros en el modelo (incluyendo el término intercepto)
num_parameters <- length(coef(lmod_reducido))

# Comparar rango con número de parámetros
matrix_rank == num_parameters

```


(b) Discutir qué soluciones existen en el sistema de ecuaciones normales cuando el número de parámetros
es igual al rango de la matriz de diseño y cuando son diferentes.


La relación entre el rango de la matriz de diseño y el número de parámetros tiene implicaciones directas en la estimabilidad de los coeficientes:

Si el rango es igual al número de parámetros, el sistema es justamente determinado y todos los parámetros pueden ser estimados sin problemas.
Si el rango es menor que el número de parámetros, esto indica multicolinealidad, lo que significa que algunos parámetros no son identificables de forma única y dependen linealmente de otros.

(c) Comprobar que los coeficientes obtenidos con las ecuaciones normales son iguales que los obtenidos
en el ejercicio 2(e).


```{r}
# Resolver las ecuaciones normales directamente
normal_eq_solution <- solve(t(design_matrix) %*% design_matrix) %*% t(design_matrix) %*% data_framingham$sysBP

# Extraer los coeficientes estimados del modelo
lm_coefficients <- coef(lmod_reducido)

# Comparar ambos métodos
all.equal(normal_eq_solution, lm_coefficients, check.attributes = FALSE)

```


(d) Obtener la estimación de la varianza del error σ2 y un intervalo de confianza al 95% suponiendo
normalidad.

```{r}
# Estimación de la varianza del error
residuals <- residuals(lmod_reducido)
estimated_variance <- sum(residuals^2) / lmod_reducido$df.residual

# Intervalo de confianza para la varianza del error
alpha <- 0.05  # Nivel de confianza del 95%
chi_sq_lower <- qchisq(alpha / 2, df = lmod_reducido$df.residual)
chi_sq_upper <- qchisq(1 - alpha / 2, df = lmod_reducido$df.residual)

variance_conf_interval <- (sum(residuals^2) / c(chi_sq_upper, chi_sq_lower))

# Mostrar resultados
list(estimated_variance = estimated_variance, variance_conf_interval = variance_conf_interval)

```